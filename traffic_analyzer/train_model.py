"""
Training script for IntelliSniff ML model.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
 - –ß–∏—Ç–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ ml/data/features.json
 - –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ –Ω–∞ —ç—Ç–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
 - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ model.joblib –≤–º–µ—Å—Ç–µ —Å–æ —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
 - –ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º —Å –Ω–æ–≤—ã–º inference.py
"""

import argparse
import os
import sys
import time
import json
import logging
import joblib
import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    confusion_matrix,
)
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight

import seaborn as sns
import matplotlib.pyplot as plt


# ============================================================================
# –ü—É—Ç–∏
# ============================================================================

BASE_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

MODEL_PATH = os.path.join(DATA_DIR, "model.joblib")
DATASET_PATH = os.path.join(BASE_DIR, "..", "datasets", "merged_detailed.parquet")
FEATURES_PATH = os.path.join(DATA_DIR, "features.json")

log = logging.getLogger("IntelliSniff.train_model")


# ============================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
# ============================================================================

def load_feature_list(path=FEATURES_PATH):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ JSON.
    –≠—Ç–æ *–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã* –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    with open(path, "r") as f:
        features = json.load(f)

    print(f"üìå –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)} —à—Ç.")
    return features


# ============================================================================
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
# ============================================================================

def load_dataset(path=DATASET_PATH, label_type="binary"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç parquet-—Ñ–∞–π–ª —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º–∏ CICIDS / VPN / Benign –¥–∞–Ω–Ω—ã–º–∏.
    """
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç: {path}")
    df = pd.read_parquet(path)

    # –í—ã–±–æ—Ä –º–µ—Ç–∫–∏
    if label_type == "multi" and "label_multi" in df.columns:
        y = LabelEncoder().fit_transform(df["label_multi"])
    elif "label_binary" in df.columns:
        y = df["label_binary"]
    elif "label" in df.columns:
        y = df["label"]
    else:
        raise ValueError("‚ùå –í –¥–∞—Ç–∞—Å–µ—Ç–µ –Ω–µ—Ç label / label_binary / label_multi")

    print(f"üîé –ù–∞–π–¥–µ–Ω–æ {df.shape[0]:,} —Å—Ç—Ä–æ–∫ –∏ {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
    return df, y


# ============================================================================
# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
# ============================================================================

def train_and_save(df, y, features, out_path=MODEL_PATH):
    """
    –û–±—É—á–∞–µ—Ç RandomForestClassifier –Ω–∞ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Å–ø–∏—Å–∫–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    """

    print("\nüß© –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏:")
    for f in features:
        print("  ‚Ä¢", f)

    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ñ–∏—á–∏ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
    missing = [f for f in features if f not in df.columns]
    if missing:
        raise ValueError(f"‚ùå –í –¥–∞—Ç–∞—Å–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏: {missing}")

    # –§–æ—Ä–º–∏—Ä—É–µ–º X
    X = df[features].fillna(0).astype(np.float32)
    print(f"üìä –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {X.shape[1]} —Ñ–∏—á–µ–π")

    # –†–∞–∑–±–∏–≤–∫–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    # –í–µ—Å—ã –∫–ª–∞—Å—Å–æ–≤
    classes = np.unique(y_train)
    class_weights = compute_class_weight("balanced", classes=classes, y=y_train)
    class_weight_dict = dict(zip(classes, class_weights))

    # –ë–∞–∑–∞ –º–æ–¥–µ–ª–∏
    rf_base = RandomForestClassifier(
        class_weight=class_weight_dict,
        random_state=42,
        n_jobs=-1,
    )

    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è RandomizedSearch
    param_dist = {
        "n_estimators": [200, 300, 400],
        "max_depth": [10, 20, 30, None],
        "min_samples_split": [2, 3, 5],
        "min_samples_leaf": [1, 2, 4],
        "max_features": ["sqrt", "log2"],
    }

    print("\nüîç –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
    search = RandomizedSearchCV(
        rf_base,
        param_distributions=param_dist,
        n_iter=8,
        cv=3,
        scoring="accuracy",
        n_jobs=-1,
        verbose=1,
        random_state=42,
    )

    search.fit(X_train, y_train)
    best_model = search.best_estimator_

    print(f"üèÜ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {search.best_params_}")

    print("üå≤ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    best_model.fit(X_train, y_train)
    y_pred = best_model.predict(X_test)

    print("\nüìä –û–¢–ß–Å–¢:")
    print(classification_report(y_test, y_pred, zero_division=0))
    print(f"üéØ Accuracy: {accuracy_score(y_test, y_pred):.4f}")

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix ‚Äî IntelliSniff Model")
    plt.xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
    plt.ylabel("–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å")
    plt.tight_layout()
    plt.savefig(os.path.join(DATA_DIR, "confusion_matrix.png"))
    print("üñºÔ∏è confusion_matrix.png —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º bundle –º–æ–¥–µ–ª–∏
    joblib.dump(
        {
            "model": best_model,
            "features": features,
            "trained_at": time.time(),
        },
        out_path
    )

    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {out_path}")
    return out_path


# ============================================================================
# MAIN
# ============================================================================

def main(argv=None):
    parser = argparse.ArgumentParser(description="–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ ML –º–æ–¥–µ–ª–∏ IntelliSniff")
    parser.add_argument("--dataset", type=str, default=None, help="–ü—É—Ç—å –∫ parquet/csv –¥–∞—Ç–∞—Å–µ—Ç—É")
    parser.add_argument("--label-type", choices=["binary", "multi"], default="binary")
    args = parser.parse_args(argv)

    dataset_path = args.dataset or DATASET_PATH

    df, y = load_dataset(dataset_path, args.label_type)
    features = load_feature_list()

    result = train_and_save(df, y, features, out_path=MODEL_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {result}")


if __name__ == "__main__":
    main(sys.argv[1:])
