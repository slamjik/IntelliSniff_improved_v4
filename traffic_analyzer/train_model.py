import argparse
import os
import sys
import time
import logging
import joblib
import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
)
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import seaborn as sns
import matplotlib.pyplot as plt


# === ÐŸÐ£Ð¢Ð˜ ===============================================================
BASE_DIR = os.path.dirname(__file__)
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

MODEL_PATH = os.path.join(DATA_DIR, "model.joblib")
DATASET_PATH = os.path.join(BASE_DIR, "..", "datasets", "merged_detailed.parquet")

log = logging.getLogger("IntelliSniff.train_model")


# === Ð—ÐÐ“Ð Ð£Ð—ÐšÐ ============================================================
def load_dataset(path=DATASET_PATH, label_type="binary"):
    """Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ parquet Ð¸ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÑ‚ X, y"""
    print(f"ðŸ“‚ Loading dataset from {path}")
    df = pd.read_parquet(path)

    # Ð²Ñ‹Ð±Ð¾Ñ€ Ð¼ÐµÑ‚ÐºÐ¸
    if label_type == "multi" and "label_multi" in df.columns:
        y = LabelEncoder().fit_transform(df["label_multi"])
    elif "label_binary" in df.columns:
        y = df["label_binary"]
    elif "label" in df.columns:
        y = df["label"]
    else:
        raise ValueError("âŒ Dataset missing label column")

    # Ð²Ñ‹Ð±Ð¾Ñ€ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²
    drop_cols = [c for c in ["label", "label_binary", "label_multi"] if c in df.columns]
    X = df.drop(columns=drop_cols, errors="ignore").select_dtypes(include=[np.number]).fillna(0).astype(np.float32)

    print(f"âœ… Dataset loaded: {X.shape[0]:,} rows, {X.shape[1]} features")
    return X, y


# === Ð ÐÐ¡Ð¨Ð˜Ð Ð•ÐÐÐžÐ• ÐžÐ‘Ð£Ð§Ð•ÐÐ˜Ð• ==============================================
def train_and_save(X, y, out_path=MODEL_PATH):
    """ÐžÐ±ÑƒÑ‡Ð°ÐµÑ‚ RandomForest Ñ Ð°Ð²Ñ‚Ð¾Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¼Ð¾Ð´ÐµÐ»ÑŒ"""
    if os.path.exists(out_path):
        try:
            os.remove(out_path)
            print(f"ðŸ§¹ Ð¡Ñ‚Ð°Ñ€Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» Ð¼Ð¾Ð´ÐµÐ»Ð¸ ÑƒÐ´Ð°Ð»Ñ‘Ð½: {out_path}")
        except PermissionError:
            print(f"âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑƒÐ´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð°Ñ€ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ. Ð—Ð°ÐºÑ€Ð¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑÑ‹ Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸.")
            return None

    print("âš™ï¸  Splitting train/test...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y if len(set(y)) > 1 else None
    )

    # ðŸ§© ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð¼ Ð²Ñ‹Ð±Ð¾Ñ€ÐºÑƒ Ð´Ð»Ñ RandomizedSearch, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ ÑƒÐ¿Ð°ÑÑ‚ÑŒ Ð¿Ð¾ Ð¿Ð°Ð¼ÑÑ‚Ð¸
    if len(X_train) > 200_000:
        print(f"âš™ï¸ Dataset too large for full search ({len(X_train):,} rows) â†’ sampling 200,000 for tuning...")
        sample_idx = np.random.choice(len(X_train), 200_000, replace=False)
        X_sample = X_train.iloc[sample_idx]
        y_sample = y_train[sample_idx]  # â† Ð²Ð¾Ñ‚ ÑÑ‚Ð¾ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ
    else:
        X_sample, y_sample = X_train, y_train

    # Ñ€Ð°ÑÑ‡ÐµÑ‚ Ð²ÐµÑÐ¾Ð² ÐºÐ»Ð°ÑÑÐ¾Ð²
    classes = np.unique(y_sample)
    from sklearn.utils.class_weight import compute_class_weight
    class_weights = compute_class_weight(class_weight="balanced", classes=classes, y=y_sample)
    class_weight_dict = dict(zip(classes, class_weights))

    print("ðŸ” Auto-tuning hyperparameters (RandomizedSearchCV)...")
    rf_base = RandomForestClassifier(class_weight=class_weight_dict, random_state=42, n_jobs=-1)
    param_dist = {
        "n_estimators": [200, 300, 400],
        "max_depth": [10, 20, 30, None],
        "min_samples_split": [2, 3, 5],
        "min_samples_leaf": [1, 2, 4],
        "max_features": ["sqrt", "log2"],
    }

    from sklearn.model_selection import RandomizedSearchCV
    search = RandomizedSearchCV(
        rf_base,
        param_distributions=param_dist,
        n_iter=8,
        cv=3,
        scoring="accuracy",
        n_jobs=-1,
        verbose=1,
        random_state=42,
    )
    search.fit(X_sample, y_sample)
    clf = search.best_estimator_

    print(f"ðŸ† Best Params: {search.best_params_}")

    print("ðŸŒ² Training optimized RandomForestClassifier on full data...")
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    print("\nðŸ“Š [TRAIN REPORT]")
    from sklearn.metrics import classification_report, accuracy_score
    print(classification_report(y_test, y_pred, zero_division=0))
    print(f"âœ… Accuracy: {accuracy_score(y_test, y_pred):.4f}")

    import seaborn as sns, matplotlib.pyplot as plt
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix â€” IntelliSniff Model")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.savefig(os.path.join(DATA_DIR, "confusion_matrix.png"))
    print(f"ðŸ–¼ï¸ Confusion matrix saved to {os.path.join(DATA_DIR, 'confusion_matrix.png')}")

    joblib.dump(
        {"model": clf, "features": X.columns.tolist(), "trained_at": time.time()},
        out_path,
    )
    print(f"ðŸ’¾ Model saved to: {out_path}")
    return out_path

    # === ÐžÐ¦Ð•ÐÐšÐ =======================================================
    print("\nðŸ“Š [TRAIN REPORT]")
    print(classification_report(y_test, y_pred, zero_division=0))
    print(f"âœ… Accuracy: {accuracy_score(y_test, y_pred):.4f}")

    # === CONFUSION MATRIX ============================================
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title("Confusion Matrix â€” IntelliSniff Model")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.savefig(os.path.join(DATA_DIR, "confusion_matrix.png"))
    print(f"ðŸ–¼ï¸ Confusion matrix saved to {os.path.join(DATA_DIR, 'confusion_matrix.png')}")

    # === Ð¡ÐžÐ¥Ð ÐÐÐ•ÐÐ˜Ð• ==================================================
    joblib.dump(
        {"model": clf, "features": X.columns.tolist(), "trained_at": time.time()},
        out_path,
    )
    print(f"ðŸ’¾ Model saved to: {out_path}")
    return out_path


# === Ð—ÐÐŸÐ£Ð¡Ðš ==============================================================
def train_from_dataset(dataset_path=None, label_type="binary", out_path=MODEL_PATH):
    dataset_path = dataset_path or DATASET_PATH
    if not os.path.exists(dataset_path):
        raise FileNotFoundError(f"Dataset not found: {dataset_path}")
    X, y = load_dataset(path=dataset_path, label_type=label_type)
    return train_and_save(X, y, out_path=out_path)


def main(argv=None):
    parser = argparse.ArgumentParser(description="Train IntelliSniff model (enhanced)")
    parser.add_argument("--dataset", type=str, default=None, help="ÐŸÑƒÑ‚ÑŒ Ðº parquet/csv Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ")
    parser.add_argument("--label-type", choices=["binary", "multi"], default="binary")
    args = parser.parse_args(argv)

    dataset_path = args.dataset or DATASET_PATH
    result = train_from_dataset(dataset_path, args.label_type)
    print(f"âœ… Model trained from {dataset_path} -> {result}")
    return result


if __name__ == "__main__":
    main(sys.argv[1:])
